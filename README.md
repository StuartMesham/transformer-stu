# Transformer-Stu

A repository for learning JAX.

## Motivation

There are two ways to achieve happiness:
1. Appreciate the beautiful world around you.
2. Write code.

As usual, I am only going to do 2, but hopefully the resulting code will help myself and others do 1.
I aim to implement a transformer model and training harness using [JAX](https://jax.readthedocs.io/en/latest/) and [Flax](https://flax.readthedocs.io/en/latest/).
I use [SentencePiece](https://github.com/google/sentencepiece) for tokenization and [tf.data](https://www.tensorflow.org/guide/data) for dataset loading and preprocessing. 
If all goes smoothly I will be able to train the model on a [positive reframing](https://github.com/SALT-NLP/positive-frames) dataset.
The resulting model should be able to translate from negative to positive language.

### Pre-Training

The positive reframing dataset is very small.
We therefore need some form of pre-training.
Options include:
- The [ParaNMT-50M](https://aclanthology.org/P18-1042.pdf) dataset of paraphrases generated by Czech-English back translation.
- [ParaBank2](https://nlp.jhu.edu/parabank/), a potential improvement over ParaNMT.
- The [MultiPIT](https://yao-dou.github.io/multipit/) datasets curated and generated from Twitter.
- Integrating weights from FLAN-T5.

## Setup
### Install Requirements
On Apple Silicon you must use Python 3.9 and run
```bash
pip install -r requirements_apple_silicon.txt
```
On any other platform, run
```bash
pip install -r requirements.txt
```

### Data

Download and extract the [positive reframing data](https://www.dropbox.com/sh/pnoczmv0uyn51e6/AAAGek6yX12Yc4PA2RwtZeZKa?dl=0) to the following directory structure:
```
data
├── dev.csv
├── test.csv
└── train.csv
```

## Run

### Train BPE Tokenizer

```bash
mkdir outputs
python flatten_data.py --input_file data/train.csv --output_file outputs/flattened_train.txt
python train_tokenizer.py
```

### Train Prefix-LM

```bash
python train.py --tokenizer_file outputs/m.model --train_data data/train.csv --val_data data/dev.csv
```

### Perform Hyper-Parameter sweep with WandB

Set WandB project name:
```bash
export WANDB_PROJECT=transformer-stu
```

Create a sweep:
```bash
python create_wandb_sweep.py
```

Launch an agent to perform training runs for the sweep:
```bash
wandb agent <username>/<project>/<sweep_id>
```
